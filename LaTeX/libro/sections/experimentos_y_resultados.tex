\section{Experimentos y Resultados}

En este capítulo se presentan los experimentos realizados, los modelos y las metodologías utilizadas en su entrenamiento, y los resultados de dichos experimentos.

\subsection{Síntesis de datos de entrenamiento}

Para las tareas de clasificación y segmentación es necesaria una verdad fundamental, ya sea una o varias etiquetas por cada imagen en el caso de la clasificación, o una máscara en el caso de la segmentación. Este es el objetivo que se quiere obtener a partir del modelo final.

Las imágenes de referencia se sintetizaron a partir de estudios de la zona del abanico aluvial del Pilcomayo \todo{ref}, las cuales se imitaron a base del mejor esfuerzo. Para facilitar la identificación visual de los paleocauces, se utilizaron imágenes de color falso, en especial utilizando imágenes de luz infrarroja (IR) de onda corta (SWIR por sus siglas en ingles, short wave infrared) utilizando las bandas B12 (IR de onda corta), B8A (IR cercano) y B4 (verde) en los canales rojo, verde y azul respectivamente. Diferentes tipos de suelo y vegetación reflejan estas bandas de formas variadas, y esta combinación puede resaltar estructuras geológicas como paleocauces al ojo humano mejor que imágenes de color real. \cite{earth-observatory-false-color}

\todo{figure of swir image and mask}

Para la creación de estas máscaras de entrenamiento se utilizaron herramientas de dibujo digital. Ejemplos de este tipo de programa incluyen {\it Krita} y {\it GIMP}.

\subsection{Herramientas utilizadas}

% python libs, cuda, docker, qgis
La herramienta fundamental utilizada en los experimentos es la librería PyTorch \cite{Ansel_PyTorch_2_Faster_2024}, que implementa una gran cantidad de componentes básicos de Machine Learning, como las capas que componen a los modelos, los conjuntos de datos de entrenamiento, funciones de activación, funciones de utilidad, entre otros.

La librería Lightning \cite{Falcon_PyTorch_Lightning_2019} expande sobre esta base, añadiendo una capa de abstracción adicional que provee bucles de entrenamiento y validación y módulos de datos que facilitan la interacción con conjuntos de datos, permitiendo al usuario enfocarse en la arquitectura del modelo y los datos de entrenamiento en lugar del proceso de entrenamiento a bajo nivel.

Finalmente, una capa de abstracción adicional sobre Lightning es proveída por TorchGeo \cite{Stewart_TorchGeo_Deep_Learning_2024}, una librería especializada para el análisis de datos de observación terrestre. TorchGeo provee facilidades para la creación de conjuntos de datos que, además de los datos multiespectrales de la imagen propia, tienen en cuenta la geolocalización de cada imagen. También proveen algunos modelos, opcionalmente preentrenados.

Para la combinación de imágenes georreferenciadas, o para la inclusión de datos de georreferenciación a imágenes que no los tienen, se utilizó la librería Rasterio \cite{gillies_2019}.

Los experimentos se realizaron exclusivamente en hardware de consumidor. Se utilizaron dos máquinas:

\begin{enumerate}
    \item Una computadora portátil con un procesador de 12 núcleos lógicos, 16GiB de memoria RAM, y una tarjeta gráfica Nvidia GeForce GTX 1050 Ti, que cuenta con 4GiB de memoria de video
    \item Una computadora de escritorio con un procesador de 12 núcleos lógicos, 32GiB de memoria RAM, y una tarjeta gráfica AMD Radeon RX 7600, que cuenta con 8GiB de memoria de video
\end{enumerate}

Cabe resaltar que los experimentos se vieron limitados por estas en su envergadura, sin embargo son reproducibles en hardware más actual o profesional. En donde sea relevante, la máquina en la cual se hizo el entrenamiento se referencia por la enumeración anterior.

\todo{add software and cuda versions in a table} % https://www.software.ac.uk/publication/how-cite-and-describe-software
% maybe appendix only

\subsection{Partición de datos de entrenamiento}

\todo{diagram of $random_grid_cell_assignment$}

El conjunto de datos en PyTorch también se conoce como {\it Dataset}, y el componente que disponibiliza estos datos como {\it DataLoader}. Lightning provee una abstracción, el {\it DataModule}, que abstrae al Dataset y permite proveer DataLoaders distintos para las tareas de entrenamiento, validación, prueba e inferencia.

Para la división de datos en datasets de entrenamiento, validación y prueba, se utilizaron métodos proveídos por TorchGeo. El procedimiento consiste en dividir los datos de entrenamiento en celdas, las cuales se asignan pseudoaleatoriamente a subconjuntos de entrenamiento, validación y prueba en una proporción de 70/20/10 respectivamente. Para cada subconjunto, luego, definimos un DataLoader que provee acceso a los datos.

Las celdas en las cuales se dividen las imágenes no se solapan, y la estrategia de cada DataLoader puede ser elegida de acuerdo al objetivo de la etapa a la cual corresponde. Para el entrenamiento usamos un muestreo aleatorio, mientras que los DataLoaders de validación y de prueba hacen un muestreo por división en celdas, asegurando que toda el área de estos conjuntos se muestrean una sola vez y que el conjunto de imágenes creado es consistente para cada tanda de entrenamiento y para cada modelo entrenado sobre el conjunto.

Cada DataLoader produce imágenes de 128$\times$128 píxeles recortadas de las celdas asignadas al DataLoader, correspondiente a un área de 7680m$\times$7680m. Las imágenes de cada tesela tienen dimensiones de 1830$\times$1830 píxeles de 60m$\times$60m, abarcando un área de $109'800$m por lado, o alrededor de 110km. La grilla en la que se dividen es de seis celdas de largo, abarcando cada celda un área cuadrada de 305 píxeles de largo, lo que permite producir 4 imágenes de validación o prueba por celda.

\subsection{Entrenamiento y validación}
\todo{clasificacion} % explain setup and failure, or only in discusion?

Para la tarea de segmentación, se utilizaron tres arquitecturas bien establecidas. Las arquitecturas proveídas por TorchGeo utilizadas fueron Foreground-Aware Relation Network, o FarSeg \cite{zheng2020foregroundawarerelationnetworkgeospatial}, y una red enteramente convolucional de cinco capas conocida simplemente como FCN por sus siglas en inglés, Fully-convolutional Network. Adicionalmente se utilizó una implementación de U-Net, una red enteramente convolucional muy conocida por sus aplicaciones en el campo de la medicina, pero tambien muy capaz en imágenes de teledetección. \cite{ronneberger2015unetconvolutionalnetworksbiomedical, khryashchev2018}

Lightning provee un componente llamado Trainer, que implementa bucles de entrenamiento, validación y prueba, y toma de entrada un modelo, un conjunto de datos, y parámetros de entrenamiento. Algunos parámetros son la cantidad mínima y máxima de épocas de entrenamiento, donde una época es una iteración del entrenamiento sobre el dataset, y una condición de detención temprana del entrenamiento si no mejora a lo largo de una cierta cantidad de tiempo.

El Trainer también brinda la posibilidad de definir un {\it checkpoint}, es decir un punto de control, que se encarga de mantener una copia de los parámetros del modelo en el punto en el que muestra el mejor rendimiento a lo largo del entrenamiento. Este checkpoint es la forma en la que el Trainer provee el modelo entrenado para luego ser usado en tareas de inferencia.

Más allá de estos procedimientos simples, la mayoría del esfuerzo fue dedicada a la creación del conjunto de datos etiquetados y a la verificación de los resultados brindados por el entrenamiento sobre estos.

\subsection{Resultados}

Para todos los experimentos, las únicas variables son los modelos usados y sus parámetros configurables, también llamados hiperparámetros. Los parámetros del Trainer y del DataModule, así como la semilla del generador de números pseudoaleatorios (PRNG) son iguales para todos los experimentos, y se detallan en el cuadro \ref{table:trainer}.

\begin{table}[ht!]
    \centering
    \begin{tabular}{ |l|r|r|p{8cm}| }
        \hline

        \rowcolor{LightBlue}
        Parámetro & Máquina 1 & Máq. 2 & Descripción \\
        \hline

        Nro. mínimo de épocas & 1 & 1 & Cantidad mínima de iteraciones por el conjunto de datos de entrenamiento. \\
        \hline

        Nro. máximo de épocas & 50 & 50 & Cantidad máxima de iteraciones por el conjunto de datos de entrenamiento.\\
        \hline

        Paciencia & 7 & 7 & Cantidad de épocas en las que el modelo no mejora la pérdida de validación antes de detener el entrenamiento. \\
        \hline

        Semilla PRNG & 42 & 42 & Valor inicial del estado del generador de números semialeatorios, empleado en, entre otras funciones, la creación de los valores iniciales de los diversos parámetros entrenables del modelo. \\
        \hline

        Tamaño de época & 1200 & 1200 & Tamaño efectivo del conjunto de datos de entrenamiento. Cantidad de imágenes, recortadas al azar de las celdas del dataset de entrenamiento, usadas para el entrenamiento en cada época. \\
        \hline

        Tamaño de imagen & 128 & 192 & Dimensión de cada recorte de los datasets en píxeles. Los recortes son cuadrados. \\
        \hline

        Tamaño de lote (batch) & 3 & 3 & Cantidad de imágenes cargadas y procesadas en cada paso de entrenamiento. Subdivide a cada época. \\
        \hline
    \end{tabular}
    \caption{Parámetros de entrenamiento generales}
    \label{table:trainer}
\end{table}

Para cada arquitectura, se realizaron experimentos con diferentes hiperparámetros, los cuales se detallan a continuación, además de resultados concretos en la forma de valores de la función de pérdida para el conjunto de prueba.

También se demuestran resultados en teselas nuevas, no utilizadas en el DataModule de entrenamiento, para demostrar las predicciones en datos completamente desconocidos. Estas teselas son aquellas cuya función es \enquote{Prueba} en el cuadro \ref{table:tiles} del capítulo anterior. En particular, la tesela T20KNC que incluye la frontera entre Paraguay y Bolivia, una sección del oeste del Parque Nacional Médanos del Chaco, y un sistema de paleocauces prominente hacia el sur del rio Parapetí en Bolivia, será utilizada como imagen de prueba.

Para la visualización de las predicciones, se convierten las matrices de salida al formato de imagen georreferenciada GeoTiff. Las predicciones de los modelos son matrices de números decimales, para los cuales un valor cercano a o mayor que $1.0$ significa una predicción de ocurrencia de paleocauce en ese píxel, mientras que un valor cercano a o menor que $0.0$ significa una predicción de ausencia de paleocauce en ese píxel. Estos valores se fijan al intérvalo $[0,1]$, con valores menores que 0 y mayores que 1 siendo convertidos a 0 y 1 respectivamente. La matriz resultante es la máscara de predicciones para una celda de $128\times128$ o $192\times192$ píxeles, y cada píxel se convierte a un valor en la escala de grises, con 0 y 1 correspondiendo a negro y blanco respectivamente.

Estas celdas de $128\times128$ o $192\times192$ píxeles se combinan para formar un mosaico de predicciones. Para mitigar los artefactos que pueden surgir en los bordes de estas celdas, estas se solapan ligeramente. Se toman pasos de 64 píxeles para predicciones de 128 y 96 píxeles para predicciones de 192, se combinan mosaicos de cada tamaño de paso por separado, y finalmente se toma un promedio de ambos mosaicos para generar una máscara de predicciones. El diagrama \todo[inline]{make diagram?} muestra el procedimiento de forma visual.

El tamaño de lote fue elegido por una limitación de memoria en la máquina 1, pero también se observó que un tamaño de lote mayor afecta negativamente al modelo resultante, inclusive en la máquina 2 con suficiente memoria.

\subsubsection{FCN}

La arquitectura más simple en este trabajo, consiste de cinco capas convolucionales con activación de tipo {\it LeakyReLU}, la cual mantiene valores positivos iguales y multiplica valores negativos por un número pequeño para minimizar su magnitud.

Los hiperparámetros seleccionados para los experimentos se detallan en el cuadro \ref{table:fcn}.

\begin{table}[ht!]
    \centering
    \begin{tabular}{ |>{\bf \columncolor{LightBlue}} c|c|c|c|c| }
        \hline

        \rowcolor{LightBlue}
        Versión & \bf Canales de entrada & \bf Nro. de filtros & \bf Mejor época & \bf Pérdida en prueba \\
        \hline

        1 & Todos (11) & 64 & 1 & 0.4675 \\
        \hline

        2 & Todos (11) & 96 & 2 & 0.4678 \\
        \hline

        3 & Todos (11) & 128 & 8 & 0.4721 \\
        \hline

        4 & Todos (11) & 256 & 4 & 0.4638 \\
        \hline

        5 & Todos (11) & 256 & 4 & 0.4630 \\
        \hline

    \end{tabular}
    \caption{Hiperparámetros y resultados de la arquitectura FCN. Entrenamiento en máquina 1.}
    \label{table:fcn}
\end{table}

Una arquitectura así de simple necesita de patrones simples y obvios para ser útil. Tal vez predeciblemente, esta arquitectura no dio resultados útiles, creando predicciones uniformes para cualquier tesela. Todos los experimentos resultan en un modelo que produce una máscara de probabilidades en la que cada píxel es negro, y la cantidad de épocas necesarias para alcanzar la menor pérdida sugiere que no se aprendieron patrones entre iteraciones.

\subsubsection{FarSeg}

La arquitectura Foreground-Aware Relation Network es una arquitectura diseñada para la segmentación geoespacial de objetos en imágenes de alta resolución espacial \cite{zheng2020foregroundawarerelationnetworkgeospatial}. Mientras que solamente utilizamos imágenes de 60m de resolución espacial, dieron resultados mucho más prometedores que FCN. Estos se muestran en el cuadro \ref{table:farseg}, además de los hiperparámetros utilizados.

La FarSeg utiliza una arquitectura de red neuronal en lo que se conoce como {\it Backbone}, o columna vertebral, en este caso ResNet \cite{he2015deepresiduallearningimage}. Esta columna cumple la función de extractor de características, las cuales son utilizadas por el resto del modelo para generar una predicción. Existe la opción de usar parámetros preentrenados para el Backbone, pero en estos experimentos se hizo el entrenamiento completo sin parámetros preentrenados.

Como la arquitectura FarSeg disponible a través de TorchGeo solo acepta tres canales de entrada, se utilizaron los conjuntos de bandas B4, B3, B2 que corresponden a los colores visibles rojo, verde, azul (RGB), y B12, B8A, B4 que corresponden a las bandas infrarrojo de onda corta o SWIR, infrarrojo de onda cercana o NIR, rojo (SWIR).

\begin{table}[ht!]
    \centering
    \begin{tabular}{ |>{\bf \columncolor{LightBlue}} c|c|c|c|c| }
        \hline

        \rowcolor{LightBlue}
        Versión & \bf Canales de entrada & \bf Backbone & \bf Mejor época & \bf Pérdida en prueba \\
        \hline

        1 & B4, B3, B2 & ResNet-18 & 1 & 0.6560 \\
        \hline

        2 & B12, B8A, B4 & ResNet-18 & 3 & 0.5866 \\
        \hline

        3 & B4, B3, B2 & ResNet-34 & 11 & 0.4682 \\
        \hline

        4 & B12, B8A, B4 & ResNet-34 & 7 & 0.5324 \\
        \hline

        5 & B4, B3, B2 & ResNet-50 & 19 & 0.4514 \\
        \hline

        6 & B12, B8A, B4 & ResNet-50 & 2 & 1.444 \\
        \hline

        7 & B4, B3, B2 & ResNet-50 & 27 & 0.4977 \\
        \hline

        8 & B12, B8A, B4 & ResNet-50 & 4 & 1.056 \\
        \hline

        9 & B4, B3, B2 & ResNet-101 & 12 & 0.5897 \\
        \hline

        10 & B12, B8A, B4 & ResNet-101 & 10 & 0.9042 \\
        \hline

        11 & B4, B3, B2 & ResNet-101 & 8 & 0.6233 \\
        \hline

        12 & B12, B8A, B4 & ResNet-101 & 5 & 0.5351 \\
        \hline

    \end{tabular}
    \caption{Hiperparámetros y resultados de la arquitectura FarSeg. Entrenamiento en máquina 1.}
    \label{table:farseg}
\end{table}

\begin{figure}
    \centering
    \subfloat[\centering Tesela T20KNC]{\includegraphics[width=0.4\textwidth]{img/T20KNC_large.png}}

    \subfloat[\centering FarSeg v. 3 (RGB)]{\includegraphics[width=0.4\textwidth]{img/farseg-34-rgb.png}}
    \quad
    \subfloat[\centering FarSeg v. 4 (SWIR)]{\includegraphics[width=0.4\textwidth]{img/farseg-34-swir.png}}

    \subfloat[\centering FarSeg v. 5 y 6 (RGB)]{\includegraphics[width=0.4\textwidth]{img/farseg-50-rgb.png}}
    \quad
    \subfloat[\centering FarSeg v. 9 y 11 (RGB)]{\includegraphics[width=0.4\textwidth]{img/farseg-101-rgb.png}}

    \caption{Máscaras de predicción de paleocauces de los modelos FarSeg}
    \label{fig:farseg}
\end{figure}

Mientras que el use de backbone más simples no dio buenos resultados, los modelos más grandes dieron resultados significantes. Interesantemente, los modelos con backbone ResNet-50 y entrenados sobre las bandas de color visible tomaron, en promedio, muchas más épocas para alcanzar su mejor rendimiento que los demás.

La figura \ref{fig:farseg} muestra las máscaras generadas por estos modelos.

Entre los resultados, los modelos con backbone ResNet-18 y ResNet-101 produjeron máscaras completamente negras, o con pocas áreas en las que predicen paleocauces. Sin embargo, aun más llamativo es el hecho que los modelos que utilizan las bandas B12, B8A, B4 producen máscaras enteramente negras, excepto con backbone ResNet-34.

Las máscaras producidas por los modelos que utilizan las bandas B4, B3, B2 presentan bordes de apariencia borrosa, lo que podría ser una consecuencia de la resolución espacial de las imágenes utilizadas.

Las máscaras producidas por los modelos entrenados con las bandas B12, B8A, B4 son similares a las producidas por FCN. Cada píxel de la máscara tiene el valor mínimo, y las pérdidas de las mejores versiones son mayores que la mayor versión de FCN.

\subsubsection{U-Net}

Una de las arquitecturas más conocidas para la tarea de segmentación es U-Net. La estructura consiste de varios niveles por los cuales desciende la imagen de entrada hasta llegar a un nivel de cuello de botella, de donde asciende nuevamente por los niveles hasta llegar a la resolución de la imagen de salida, que en este caso es igual a la resolución de la imagen de entrada.

En cada descenso a un nivel inferior, los datos crecen en la cantidad de canales y decrecen en la resolución de cada canal, es decir, la imagen disminuye en su tamaño. Inversamente, al ascender a un nivel superior, disminuye la cantidad de canales e incrementa la resolución de cada canal. El cuello de botella es el último nivel, en el cual la resolución es mínima y la cantidad de canales es máxima.

En adición a las conexiones entre niveles adyacentes, existen conexiones entre los caminos descendientes y ascendientes del mismo nivel, saltando niveles en estas conexiones.

Cada nivel tiene un tamaño que describe la cantidad de filtros, y debe ser el doble que el nivel anterior o la mitad del nivel siguiente. Cada nivel disminuye y aumenta la resolución de la imagen por un factor de 4, o sea un factor de 2 por cada dimensión.

Los hiperparámetros utilizados y los resultados de los experimentos se visualizan en los cuadros \ref{table:unet1} y \ref{table:unet2}.

\begin{table}[ht!]
    \centering
    \begin{tabular}{ |>{\bf \columncolor{LightBlue}} c|c|c|c|c| }
        \hline

        \rowcolor{LightBlue}
        Versión & \bf Canales de entrada & \bf Niveles & \bf Mejor época & \bf Pérdida en prueba \\
        \hline

        1 & Todos (11) & 64, 128, 256, 512 & 10 & 0.4421 \\
        \hline

        2 & Todos (11) & 64, 128, 256, 512 & 10 & 0.4576 \\
        \hline

        3 & Todos (11) & 32, 64, 128, 256, 512 & 14 & 0.4535 \\
        \hline

        4 & Todos (11) & 32, 64, 128, 256, 512 & 3 & 0.4520 \\
        \hline

        5 & Todos (11) & 64, 128, 256, 512, 1024 & 10 & 0.5412 \\
        \hline

        6 & Todos (11) & 64, 128, 256, 512, 1024 & 10 & 0.4426 \\
        \hline

    \end{tabular}
    \caption{Hiperparámetros y resultados de la arquitectura U-Net para imágenes de $128\times128$. Entrenamiento en máquina 1.}
    \label{table:unet1}
\end{table}

\begin{table}[ht!]
    \centering
    \begin{tabular}{ |>{\bf \columncolor{LightBlue}} c|c|c|c|c| }
        \hline

        \rowcolor{LightBlue}
        Versión & \bf Canales de entrada & \bf Niveles & \bf Mejor época & \bf Pérdida en prueba \\
        \hline

        7 & Todos (11) & 64, 128, 256, 512 & 11 & 0.4385 \\
        \hline

        8 & Todos (11) & 64, 128, 256, 512 & 6 & 0.4138 \\
        \hline

        9 & Todos (11) & 32, 64, 128, 256, 512 & 12 & 0.4205 \\
        \hline

        10 & Todos (11) & 32, 64, 128, 256, 512 & 12 & 0.4205 \\
        \hline

        11 & Todos (11) & 64, 128, 256, 512, 1024 & 12 & 0.4250 \\
        \hline

        12 & Todos (11) & 64, 128, 256, 512, 1024 & 16 & 0.4172 \\
        \hline

    \end{tabular}
    \caption{Hiperparámetros y resultados de la arquitectura U-Net para imágenes de $192\times192$. Entrenamiento en máquina 2.}
    \label{table:unet2}
\end{table}

\begin{figure}
    \centering
    \subfloat[\centering Tesela T20KNC]{\includegraphics[width=0.5\textwidth]{img/T20KNC_large.png}}

    \subfloat[\centering U-Net v. 1 y 2]{\includegraphics[width=0.3\textwidth]{img/unet-1_1-2.png}}
    \quad
    \subfloat[\centering U-Net v. 3 y 4]{\includegraphics[width=0.3\textwidth]{img/unet-1_3-4.png}}
    \quad
    \subfloat[\centering U-Net v. 5 y 6]{\includegraphics[width=0.3\textwidth]{img/unet-1_5-6.png}}

    \subfloat[\centering U-Net v. 7 y 8]{\includegraphics[width=0.3\textwidth]{img/unet-2_7-8.png}}
    \quad
    \subfloat[\centering U-Net v. 9 y 10]{\includegraphics[width=0.3\textwidth]{img/unet-2_9-10.png}}
    \quad
    \subfloat[\centering U-Net v. 11 y 12]{\includegraphics[width=0.3\textwidth]{img/unet-2_11-12.png}}

    \caption{Máscaras de predicción de paleocauces de los modelos U-Net}
    \label{fig:unet}
\end{figure}

La pérdida de estos modelos es en promedio menor que la pérdida promedio de los modelos anteriores, y las máscaras generadas son mucho más detalladas. Estas máscaras se muestran en la figura \ref{fig:unet}.

Observando los resultados producidos por los modelos entrenados, es evidente que el análisis de imágenes más pequeñas produce resultados más limitados, en donde solamente áreas con un contraste más pronunciado son detectadas como áreas de paleocauces, mientras que el análisis de imágenes que cubren un área más extensa producen predicciones más similares a las máscaras de entrenamiento. Las versiones 11 y 12 son las únicas que producen una predicción sustancial para los paleocauces a lo largo del margen izquierdo de la imagen.

Una ventaja de las versiones 1-6 es que sus predicciones son más consistentes, con las versiones 7-12 dependiendo mucho más de la composición de los niveles.

\subsubsection{Predicción en conjunto}

Un problema con las máscaras generadas por los modelos es la cantidad de artefactos y diferencias en los resultados, aun con la misma arquitectura y la misma distribución de datos de entrenamiento. Por esta razón, también experimentamos con la creación de máscaras usando un conjunto de modelos, cuyos resultados fueron adicionados para generar una sola máscara. Esto permite generar mapas de calor más consistentes, en los cuales los puntos ciegos de un modelo pueden ser cubiertos por los puntos fuertes de otro.

Aun con las diferencias entre los modelos, es común encontrar zonas en las cuales ningún modelo predice ocurrencias de paleocauces, mientras que en otras zonas la mayoría predice positivamente con alta certeza.

\begin{figure}
    \centering
    \subfloat[\centering Combinación FarSeg]{\includegraphics[width=0.4\textwidth]{img/farseg-combined.png}}
    \quad
    \subfloat[\centering Tesela T20KNC con máscara combinada FarSeg]{\includegraphics[width=0.4\textwidth]{img/farseg-combined-screen.png}}

    \subfloat[\centering Combinación U-Net 1-6]{\includegraphics[width=0.4\textwidth]{img/unet-combined.png}}
    \quad
    \subfloat[\centering Tesela T20KNC con máscara combinada U-Net 1-6]{\includegraphics[width=0.4\textwidth]{img/unet-combined-screen.png}}

    \subfloat[\centering Combinación U-Net 7-12]{\includegraphics[width=0.4\textwidth]{img/unet-combined-large.png}}
    \quad
    \subfloat[\centering Tesela T20KNC con máscara combinada U-Net 7-12]{\includegraphics[width=0.4\textwidth]{img/unet-combined-large-screen.png}}

    \caption{Máscaras de predicción producidas por una combinación de las predicciones anteriores}
    \label{fig:combined}
\end{figure}

La figura \ref{fig:combined} muestra máscaras generadas por una combinación de las predicciones de varios modelos, presentando predicciones más completas.

\todo[inline]{add section about argentina and neuland predictions in another subsection}
